1. 强化学习的基本过程

   (1) 定义

- RL是一个智能体（agent），这个智能体具有行动（action）的能力；

- 每一步的行动都会影响智能体将来的状态（state）；

- 成功与否通过一个标量的奖励信号(reward)进行衡量；

- 目标是为了选择一组动作，使得得到的奖励(reward)最大化。

  (2) 三要素

* 策略 (policy)：智能体的行为函数；

* 价值函数 (Value Function)：描述每一个状态或者是动作到底有多好；

* 模型：智能的环境表示。通常在实际问题当中是学习的结果。

  (3) 目标

  强化学习解决序列决策问题

  

2. 如何理解策略梯度算法，在你的论文中策略梯度算法具体是怎么用的

   (1) 优点：

- 具有更好的收敛性；

- 对于高维空间或者是连续空间更加的有效；     

- 能够对随机策略进行学习。

  (2) 缺点： 

- 很容易在局部最优解上面收敛而得不到全局最优；

- 对策略的估计通常具有很大的方差，求解的过程较低效。

  (3) 策略目标函数

* 所有的最优策略有相同的最优价值函数

* 找到**最优值函数**，就能对应找到**最好策略**。

 

- start  value是针对拥有起始状态的情况下求起始状态获得的奖励

- average  value针对不存在起始状态而且停止状态也不固定的情况，在这些可能的状态上计算平均获得的奖励

- Average reward per time-step为每一个时间步长在各种情况下所能得到的平均奖励

   (4) 目标

  策略梯度的优化问题，找到参数θ来最大化目标函数

  (5) 单步MDP问题

  目标函数的梯度等于**策略函数对数梯度**与**即时奖励**两部分乘积的期望

  在回报reward中引入常数基线b的方式来减小方差。

  

2. 如何理解online policy 和 offline policy

   行为策略是用来与环境互动产生数据的策略，即**在训练过程中做决策**；

   目标策略在行为策略产生的数据中不断学习、优化，即**学习训练完毕后拿去应用的策略**

   

   区别就是在于更新值函数时的策略

   * on-policy，只使用了当前策略产生的样本，更新Q值时所使用的方法是沿用既定的策略
   * off-policy，并不一定使用当前策略产生的样本，更新Q值时所使用的方法是使用新策略



3. 如何理解Policy Gradient 和 Q Learning的区别

   (1) Policy Gradient

   * 直接学习参数化的策略，动作选择不再依赖值函数。
   * 通过对策略函数进行梯度上升算法最大化reward
   * 策略搜索强化学习方法

   

   (2) Q-learning

* 需要**学习动作价值函数（Value Function）**，进行on-pilicy learning得到optimal的选择

* 基于值函数估计的强化学习方法

  简单说来，Policy Gradient直接学习策略，Q-learning学习Q值，根据Q值来选择动作

 

4. Model Base和Model free有什么区别