1. attention的结构，Encoder-Decoder的结构，attention怎么工作？
2. attention机制 self-attention还有multi-head
3. b-attention和L-attention的不同
4. Transformer的结构图，讲一下原理，为什么self-attention可以替代seq2seq
5. 机器翻译的Attention机制，里面的q,k,v分别代表什么
6. transformer中句子的encoder表示是什么；怎么加入词序信息的
7. Attention模型和CNN 的区别？
8. Transformer结构，input_mask如何作用到后面self-attention计算过程。Transformer中的三种mask的区别
9. 讲transformer如何并行化运算，self-attention和普通seq2seq的attention区别
10. 前馈神经网络有没有隐藏层
11. 如何理解Transformer中的位置信息，后续有什么改进
12. Transformer embedding部分为什么用+不用concat
13. 注意力机制介绍的原理与数学推导
14. self-attention和attention的区别
15. local attention和global attention的区别
16. 对Transformer的改进模型你了解多少，为什么改进后会有效果
17. Transformer 用的 Layer Normalize 还是 Batch Normalize？有什么区别
18. point-wise，pair-wise, list-wise的优缺点，对这些loss的常用设计形式了解吗？
19. Transformer中的Attention机制
20. Transformer的Attention，为什么要除以根号d