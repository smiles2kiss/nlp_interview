1. 解决过拟合的方法
	
	* 获得更多的数据，数据增强
	* 使用合适的模型，减少网络的层数、神经元个数等均可以限制网络的拟合能力
	* 调整训练时间，Early stopping（早停）
	* 限制权值 Weight-decay，也叫正则化（regularization）
	  * 过拟合，就是拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。
	  * 正则化是通过约束参数的范数使其不要太大，所以可以在一定程度上减少过拟合情况。
	* L1正则化
	  * 加了先验。在数据少的时候，先验知识可以防止过拟合
	  * 过拟合的时候，拟合函数的系数往往非常大
	  * 正则化是通过约束参数的范数使其不要太大，所以可以在一定程度上减少过拟合情况。
	  * 一句话理解: 通常神经网络可以模拟任何非线性函数，即通过增加隐含层的数量来无限接近目标值，然而，这样的情况下，难免也会把噪声进行了拟合，即为了避免过拟合，需要将权重正则化。权重正则化的含义就是保证权重系数在绝对值意义上足够小，使得噪声不会被较好地拟合，噪声相比于正常信号而言，通常会在某些点出现较大的峰值，因此通过权重正则化从而实现了较好的模型建立。
	
	* 增加噪声 Noise
	  * 在输入中添加噪声
  * 在权值中添加噪声
	  * 在网络响应中添加噪声
	* 集成学习：Bagging 和 Boosting
	* Dropout
	  * 在训练时，每次随机概率p（如50%概率）忽略隐层的某些节点；
	  * 在测试时，所有节点乘上概率P
	* 贝叶斯方法



2. 解决梯度消失的方法
   * 预训练加微调
   * 梯度剪切、权值正则化
   * sigmoid函数替换为relu、leakrelu、elu等激活函数
   * batchnorm：即批规范化，通过规范化操作将输出信号x规范化保证网络的稳定性。
   * 残差结构
   * LSTM



3. ResNet的改进位置

4. 模型训练的停止标准是什么？如何确定模型的状态（指标不再提升）

5. 如何解决模型不收敛问题 以及如何加快模型的训练速度

6. 深度学习模型在训练过程中如何加速收敛

7. 模型学习中，正负样本的训练方式不同有什么影响

8. 如何做标签平滑

9. 在训练阶段和测试阶段，Batch Normalization和Dropout有什么不同