1. 常用优化器

  * 梯度下降法(Gradient Descent)
    * 标准梯度下降法(GD)
    * 批量梯度下降法(BGD)
    * 随机梯度下降法(SGD)
    * 动量优化法(SGD Momentum)
    * 牛顿加速梯度法(NAG)
  * 自适应学习率优化算法
    * AdaGrad算法
    * RMSProp算法
    * Adam算法
    * AdaDelta算法
    * AdaMax算法
    * Nadam算法
    * AdamW算法
    * AMSGrad算法
  * 最新优化算法
    * BertAdam优化器
    * RAdam优化器
    * LookAhead优化器
    * Ranger优化器
    * AdaBelief优化器

  

2. 介绍优化器的作用，以及不同优化器的区别Adam，SGD，BertAdam

3. 描述优化器的进化过程
从动量的角度
从学习率的角度

4. 为什么要Warmup
	
	* 有助于减缓模型在初始阶段对mini-batch的提前过拟合现象，保持分布的平稳
* 有助于保持模型深层的稳定性
	
5. SGD 和 mini-SGD的区别

6. 对epoch做shuffle，类似于哪一种优化器

7. 什么情况下不适用动量优化器，WGAN要保证梯度平滑，使动量优化器容易过拟合，防止梯度突变

8. Adam如何设置参数使学习率衰减？

9. 深度学习里面的优化方法momentum和Adam来分别讲一下原理和公式
