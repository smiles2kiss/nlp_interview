1. 为什么RNN比CNN和全连接层神经网络更容易发生梯度消失或爆炸	
2. 如何解决sigmoid函数饱和后的梯度消失问题
3. 梯度消失爆炸的原因怎么解决
4. 解决梯度爆炸的方式（算法层面)
5. Batch Normalization为什么能够解决梯度爆炸问题
6. 梯度消失问题和损失函数有关吗？