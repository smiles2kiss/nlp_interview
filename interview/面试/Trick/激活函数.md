1. 常用激活函数

  * Sigmoid函数

  * Tanh函数

  * ReLU函数

  * pReLU函数

  * ELU函数

  * Leaky ReLU函数

  * GELU函数

  * Maxout函数

  * Softmax函数

    

2. 神经网络中激活函数的作用是什么
   线性模型的表达能力不够，因此引入激活函数来向神经网络中加入非线性因素。

   
3. 激活函数有哪些以及他们的区别
4. 激活函数无限维的问题
5. Sigmoid 和 ReLU 区别，ReLU 解决了什么问题。
6. 常用激活函数，sigmoid，tanh，ReLU, leaky ReLU, PReLU, ELU，random ReLU等
7. RELU的缺点是什么，如何解决
8. 为什么用sigmoid函数进行非线性映射（从二项分布的伯努利方程角度）