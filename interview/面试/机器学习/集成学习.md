1. GBDT和逻辑回归的区别
2. GBDT和Xgboost的区别，XGBoost相比于GBDT有哪些改进
3. 如何改进和提升Xgboost模型
4. 介绍一下LightGBM与Xgboost的区别
5. GBDT推导
6. Xgboost原理，怎么防过拟合
7. boosting和bagging在不同情况下的选用
8. Adaboost和XGBoost的区别，Adaboost和XGBoost是怎么进行预测的
9. LightGBM和XGBoost，GBDT的区别（LGB=GBDT+GOSS+EFB）
10. DEEP&WIDE模型，DEEP FM知道吗？
11. Adaboost，gbdt，xgboost，从损失函数，基学习器，训练方式等方面说明，写了xgboost的泰勒展开
12. Gbdt是怎么选择特征的，答借助于CART树模型进行选择，类似于ID3,C4,5用信息增益和信息增益率。GBDT还可以构建特征什么的
13. XGBOOST ，LGB 生长策略，分类策略
14. Xgboost怎么解决拟合和泛化问题的
15. GBDT是否只能用CART树，GBDT中残差计算公式
16. lightgbm的直方图加速讲一下？具体是怎么来做的？
17. lightgbm的叶子节点是怎么分裂的？说一下
18. 集成学习的方法有哪些
	Bagging 和 Boosting

	Bagging 
	思路
	所有基础模型都一致对待，每个基础模型手里都只有一票。然后使用民主投票的方式得到最终的结果。
	经过 bagging 得到的结果方差（variance）更小。
	具体过程
	(1) 从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）
	(2) 每次使用一个训练集得到一个模型，k个训练集共得到k个模型。
	(3) 对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。
	
	Boosting
	思路
	是经过不停的考验和筛选来挑选出「精英」，然后给精英更多的投票权，表现不好的基础模型则给较少的投票权，然后综合所有人的投票得到最终结果。
	
	经过 boosting 得到的结果偏差（bias）更小
	具体过程
	(1) 通过加法模型将基础模型进行线性的组合。
	(2) 每一轮训练都提升那些错误率小的基础模型权重，同时减小错误率高的模型权重。
	(3) 在每一轮改变训练数据的权值或概率分布，通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。
	
	二者的区别
	(1) 样本选择上
	Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。
	Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。
	(2) 样例权重
	Bagging：使用均匀取样，每个样例的权重相等
	Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。
	(3) 预测函数
	Bagging：所有预测函数的权重相等
	Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。
	(4) 并行计算
	Bagging：各个预测函数可以并行生成
Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。